# InferX

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


InferX is model wrapper tool we've been using internally to easily test and benchmark ML models across various hardware configurations. It automatically detects your hardware and prepares and executes model inference based on that device. 

This was mainly built to test models on A100/H100s and also Jetsons. Though this can be extended to support any device.

ğŸ“š **[View Full Documentation](https://docs.exla.ai/)**

## Key notes:
1. There are different implementations of models.
2. A keynote with InferX is we make sure that only libraries that you need to install per library 

## ğŸš€ Quick Start

1. **Add your user to the Docker group**
   ```bash
   sudo usermod -aG docker $USER
   ```
   **âš ï¸ Restart your terminal** for this to take effect.

2. **Install UV**
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

3. **Clone the examples repository**:
   ```bash
   git clone https://github.com/exla-ai/inferx-examples.git
   cd inferx-examples
   ```

4. **Install InferX**:
   ```bash
   uv pip install git+https://github.com/exla-ai/InferX.git
   ```

5. **Activate the virtual environment**:
   ```bash
   source .venv/bin/activate
   ```

6. **Run your first example**:
   ```bash
   python clip/example_clip.py
   ```

ğŸ‰ **That's it!** InferX will automatically detect your hardware and run an optimized instance of the model. 

## Examples

Every supported model has a corresponding example here: https://github.com/exla-ai/inferx-examples

## ğŸ¯ Model & Platform Compatibility Matrix

| Model | Jetson AGX Orin | Jetson Orin Nano | NVIDIA H100 | NVIDIA A100 | CPU (x86-64) | CPU (ARM64) |
|-------|----------------|------------------|--------------|-------------|---------------|-------------|
| **CLIP** | âœ… Tested | âœ… Tested | âœ… Tested | âœ… Tested | âœ… Tested | ğŸŸ¡ Compatible |
| **SAM2** | âœ… Tested | âœ… Tested | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible* | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible |
| **RoboPoint** | âœ… Tested | âœ… Tested | âœ… Tested | âœ… Tested | âœ… Tested | ğŸŸ¡ Compatible |
| **Whisper** | âœ… Tested | âœ… Tested | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible | âœ… Tested | ğŸŸ¡ Compatible |
| **DeepSeek R1** | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible | âœ… Tested | âœ… Tested | âœ… Tested | ğŸŸ¡ Compatible |
| **ResNet34** | âœ… Tested | âœ… Tested | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible | âœ… Tested | ğŸŸ¡ Compatible |
| **MobileNet** | âœ… Tested | âœ… Tested | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible| âœ… Tested | ğŸŸ¡ Compatible |
| **InternVL2.5** | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible | ğŸŸ¡ Compatible | âœ… Tested | ğŸŸ¡ Compatible| ğŸŸ¡ Compatible |

### **Legend:**
- âœ… **Tested**: Verified working with full optimizations
- ğŸŸ¡ **Compatible***: Should work based on hardware specs, not extensively tested  
- âŒ **Not Supported**: Hardware limitations

## How It Works

Inferx uses "extreme lazy loading" to only download packages and dependencies you actually need:

1. **Hardware Detection**: The library automatically detects your hardware and selects the appropriate implementation.
2. **On-demand Dependencies**: Dependencies are only installed when needed, reducing installation footprint.
3. **Automatic Setup**: Environment variables and configurations are set up automatically.
4. **Graceful Fallbacks**: If GPU acceleration isn't available, the library *should* (might not be the case for all models currently) falls back to CPU.

## ğŸ—ï¸ Project Structure

```
inferx/
â”œâ”€â”€ models/                    # Model implementations organized by model type
â”‚   â”œâ”€â”€ clip/                  # CLIP multimodal model (example)
â”‚   â”‚   â”œâ”€â”€ __init__.py       # Main CLIP interface
â”‚   â”‚   â”œâ”€â”€ README.md         # CLIP-specific documentation
â”‚   â”‚   â””â”€â”€ _implementations/ # Hardware-specific implementations
â”‚   â”‚       â”œâ”€â”€ __init__.py   # Implementation selector logic
â”‚   â”‚       â”œâ”€â”€ clip_gpu.py   # NVIDIA GPU implementation (CUDA + TensorRT)
â”‚   â”‚       â”œâ”€â”€ clip_jetson.py # Jetson-optimized implementation
â”‚   â”‚       â””â”€â”€ clip_cpu.py   # CPU fallback implementation
|   |       ...
â”œâ”€â”€ optimize/                  # Model optimization utilities
â”œâ”€â”€ utils/                    # Hardware detection and utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ device_detect.py      # Hardware detection logic
â”‚   â”œâ”€â”€ docker_utils.py       # Docker container management
â”‚   â”œâ”€â”€ download_utils.py     # Model download and caching
â”‚   â””â”€â”€ config.py             # Configuration management
â”œâ”€â”€ docker/                   # Docker configurations and tools
â”‚   â”œâ”€â”€ gpu/                  # GPU-specific Docker configs
â”‚   â”œâ”€â”€ jetson/               # Jetson-specific Docker configs
â”‚   â””â”€â”€ cpu/                  # CPU-specific Docker configs
```


## ğŸ¤ Contributing

We welcome contributions! 

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“§ Support

Having issues? Reach out to us at [contact@exla.ai](mailto:contact@exla.ai)

---

**Made with â¤ï¸ for the ML community from Exla**
