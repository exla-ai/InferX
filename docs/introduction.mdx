---
title: 'Introduction to Exla'
description: 'Hardware-aware AI model optimization for edge devices'
---

# Welcome to Exla

Exla is an advanced model optimization platform that makes AI models smaller, faster, and more deployable on constrained devices. We provide hardware-aware model optimization, enabling developers to deploy efficient, production-ready models with just a few lines of code.

## Key Features

- **Hardware-Aware Optimization**: Automatically optimizes models based on target hardware specifications
- **Mixed Precision Quantization**: Adaptive bit-width precision for optimal performance
- **Structured Pruning**: Intelligent removal of redundant computations while preserving accuracy
- **Kernel Fusion**: Advanced optimizations for reduced memory overhead and improved inference speed
- **Simple Integration**: Deploy optimized models with minimal configuration
- **Multiple Hardware Support**: Optimized for various GPUs and edge devices

## Why Exla?

Optimizing models for edge devices like NVIDIA Jetsons and Raspberry Pis is notoriously complex and time-consuming. Exla automates this process, making it possible to run large models on low-power hardware without sacrificing performance.

## Getting Started

### Installation

1. First, request an access token by emailing us at contact@exla.ai

2. Once you have your token, install Exla using `uv`:
```bash
uv pip install --index-url https://pypi.exla.ai/<your-token>/simple exla
```

### Basic Usage

#### Running DeepSeek
```python
from exla.models import DeepSeekR1

# Initialize the model
model = DeepSeekR1()

# Generate text
response = model.generate("Tell me about artificial intelligence")
print(response)
```

#### Running RoboPoint
```python
from exla.models.robopoint import robopoint

# Initialize the model
model = robopoint()

# Run inference with a text instruction
model.inference(
    image_path="path/to/image.jpg",
    text_instruction="Find a few spots where I could place an object.",
    output="path/to/output.png"
)
```

#### Running Your Custom Model
```python
from exla import Model
from exla.config import ModelConfig

# Define your model configuration
config = ModelConfig(
    model_path="path/to/your/model",
    model_type="transformer",  # or "vision", "diffusion", etc.
    target_device="gpu"  # or "cpu", "orin_nano", etc.
)

# Initialize and run your model
model = Model(config)
output = model.run(your_input)
```

For more detailed examples and advanced usage, check out our [Quickstart Guide](/quickstart) or explore our [Hardware & Model Compatibility](/compatibility).

## Connect With Us

- Email: contact@exla.ai
- Twitter: [@exla_ai](https://x.com/exla_ai)
- LinkedIn: [Exla](https://linkedin.com/company/106019408) 