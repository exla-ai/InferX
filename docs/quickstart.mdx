---
title: 'Quickstart'
description: 'Get started with Exla SDK in minutes'
---

# Quickstart Guide

This guide will help you get started with the Exla SDK quickly. We'll walk through installation and run a simple example using the CLIP model.

## Installation

Install the Exla SDK using pip:

```bash
uv pip install git+https://<ACCESS_TOKEN>@github.com/exla-ai/exla-sdk.git
```

For Jetson devices, we recommend using Python 3.10 for optimal performance:

```bash
# Create a Python 3.10 virtual environment
python3.10 -m venv exla-env
source exla-env/bin/activate

# Install Exla SDK
pip install exla-sdk
```

## Running Your First Model

Let's run a simple example using the CLIP model, which can match images with text descriptions.

### 1. Prepare Your Images

Save some test images to work with. For this example, we'll use two images:

```bash
# Create a data directory
mkdir -p data

# Download sample images
wget -O data/dog.jpg https://images.unsplash.com/photo-1543466835-00a7907e9de1
wget -O data/cat.jpg https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba
```

### 2. Create a Simple Script

Create a file named `clip_example.py` with the following content:

```python
from exla.models.clip import clip
import json

# Initialize the model (automatically detects your hardware)
model = clip()

# Run inference
results = model.inference(
    image_paths=["data/dog.jpg", "data/cat.jpg"],
    text_queries=["a photo of a dog", "a photo of a cat", "a photo of a bird"]
)

# Print results
print(json.dumps(results, indent=2))
```

### 3. Run the Script

```bash
python clip_example.py
```

You'll see real-time progress with animated spinners as the model loads and runs inference:

```
‚ú® EXLA SDK - CLIP Model ‚ú®
üîç Device Detected: AGX_ORIN

üìä Initial System Resources:
üìä Resource Monitor - NVIDIA Jetson AGX Orin
üíª System Memory: 4.2GB / 15.6GB (27%)

‚†è [0.5s] Initializing Exla Optimized CLIP model for AGX_ORIN [GPU Mode]
‚úì [0.6s] Initializing Exla Optimized CLIP model for AGX_ORIN [GPU Mode]

üöÄ Running CLIP inference on your images
‚úì [0.2s] Processed 2 images
‚†ã [1.2s] Loading CLIP model
Using GPU: Orin
‚úì [4.1s] Model ready on CUDA
‚†ã [0.0s] Running CLIP inference
‚úì [0.8s] Inference completed successfully
‚úì [0.1s] Processing results

‚ú® CLIP Inference Summary:
   ‚Ä¢ Model: openai/clip-vit-large-patch14-336
   ‚Ä¢ Device: CUDA
   ‚Ä¢ Images processed: 2
   ‚Ä¢ Text queries: 3
   ‚Ä¢ Total time: 5.17s
```

### 4. Understand the Results

The output will be a JSON structure showing how well each image matches each text query:

```json
[
  {
    "a photo of a dog": [
      {
        "image_path": "data/dog.jpg",
        "score": "23.1011"
      },
      {
        "image_path": "data/cat.jpg",
        "score": "17.1396"
      }
    ]
  },
  {
    "a photo of a cat": [
      {
        "image_path": "data/cat.jpg",
        "score": "25.3045"
      },
      {
        "image_path": "data/dog.jpg",
        "score": "18.7532"
      }
    ]
  },
  {
    "a photo of a bird": [
      {
        "image_path": "data/dog.jpg",
        "score": "15.9280"
      },
      {
        "image_path": "data/cat.jpg",
        "score": "15.9280"
      }
    ]
  }
]
```

Higher scores indicate better matches between the image and text.

## Try the RoboPoint Model

Let's also try the RoboPoint model, which can predict keypoint affordances in images based on natural language instructions.

### 1. Prepare Your Images

Save a test image to work with:

```bash
# Create a data directory if you haven't already
mkdir -p data

# Download a sample image
wget -O data/table.jpg https://images.unsplash.com/photo-1565791380713-1756b9a05343
```

### 2. Create a Simple Script

Create a file named `robopoint_example.py` with the following content:

```python
from exla.models.robopoint import robopoint
import os

# Initialize the model (automatically detects your hardware)
model = robopoint()

# Ensure output directory exists
os.makedirs("data", exist_ok=True)

# Run inference with a text instruction
result = model.inference(
    image_path="data/table.jpg",
    text_instruction="Find a few spots where I could place a cup on the empty area of the table.",
    output="data/table_output.png"
)

# Process the results
if result["status"] == "success":
    print(f"‚úÖ Successfully detected {len(result['keypoints'])} keypoints")
    print(f"üìä Keypoints (normalized coordinates):")
    for i, (x, y) in enumerate(result['keypoints']):
        print(f"   Point {i+1}: ({x:.4f}, {y:.4f})")
    print(f"üñºÔ∏è Visualization saved to: data/table_output.png")
else:
    print(f"‚ùå Inference failed: {result.get('error', 'Unknown error')}")
```

### 3. Run the Script

```bash
python robopoint_example.py
```

You'll see real-time progress with animated spinners as the model loads and runs inference:

```
‚ú® EXLA SDK - RoboPoint Model ‚ú®
üîç Device Detected: GPU

‚†è [0.5s] Checking Docker availability
‚úì [0.6s] Checking Docker availability
‚†ã [1.2s] Pulling Exla Optimized RoboPoint Docker image
‚úì [4.1s] Pulling Exla Optimized RoboPoint Docker image

üöÄ EXLA Optimized RoboPoint - Vision-Language Keypoint Prediction
üì∑ Processing image: table.jpg
üí¨ Instruction: Find a few spots where I could place a cup on the empty area of the table.
üìÅ Output will be saved to: data/table_output.png
‚†ã [0.0s] Running RoboPoint inference on GPU
‚úì [2.8s] Running RoboPoint inference on GPU

‚úÖ Successfully detected 5 keypoints
üìä Keypoints (normalized coordinates):
   Point 1: (0.4231, 0.5642)
   Point 2: (0.6754, 0.4987)
   Point 3: (0.5123, 0.6231)
   Point 4: (0.3987, 0.4532)
   Point 5: (0.7123, 0.5876)
üñºÔ∏è Visualization saved to: data/table_output.png
```

The model will process the image and generate an output image with keypoints visualized based on your text instruction.

## Next Steps

- Explore the [CLIP model documentation](/models/clip) for advanced usage
- Try the [RoboPoint model](/models/robopoint) for keypoint affordance prediction
- Try other models in the Exla SDK
- Check out the [compatibility guide](/compatibility) for hardware-specific optimizations 