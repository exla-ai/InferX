---
title: 'Serving Pre-built Models'
description: 'Run optimized off-the-shelf models with Exla'
---

# Serving Pre-built Models

Exla provides optimized versions of popular AI models that are ready to use out of the box. This guide shows you how to quickly get started with pre-built models like DeepSeek.

## Installation

First, install Exla using pip:

```bash
pip install exlasdk
```

## Running DeepSeek

The fastest way to get started is using the Exla CLI:

```bash
exla run deepseek-r1x
```

This command will:
1. Download the optimized DeepSeek model
2. Configure it for your hardware
3. Start a local instance

### Using DeepSeek in Python

For more control, you can use the Python API:

```python
from exla.models.deepseek_r1 import deepseek_r1

# Initialize the model - automatically optimized for your hardware
model = deepseek_r1()

# Generate text
response = model.run("Explain quantum computing in simple terms")
print(response)
```

## Available Models

Exla currently supports these optimized models:

### DeepSeek
- State-of-the-art language model
- Optimized for various tasks like text generation and completion
- [Learn more about DeepSeek](/models/deepseek)

### CLIP
- Powerful vision-language model
- Great for image understanding and similarity search
- [Learn more about CLIP](/models/clip)

## Hardware Support

Pre-built models are automatically optimized for:
- NVIDIA GPUs with CUDA support
- Edge devices (NVIDIA Jetson)
- CPU deployment

The optimization process is automatic - Exla detects your hardware and applies the right optimizations.

## Next Steps

- Try [optimizing your own models](/guides/custom-models)
- Explore our [model reference](/models/deepseek)
- Join our community on [Twitter](https://x.com/exla_ai) 